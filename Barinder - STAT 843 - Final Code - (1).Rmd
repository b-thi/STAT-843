---
title: Functional Neural Networks Code - Feed Forward, Scalar Response, Functional
  Covariate
author: "Barinder Thind - 301193363"
date: "April 20, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

-------------------------------------------------------------------------------------------------

This document provides functions and an illustration of a feed-forward functional neural network with evidence of a lower error rate than regular functional linear regression. This is initial work and the functions are geared towards a specific toy example. All relevant details are provided in the accompanying report.

-------------------------------------------------------------------------------------------------

Initialize with relevant libraries:

```{r}
library(tidyverse)
library(fda)
library(refund)
library(fda.usc)
```


First, a function that computes an approximation of an integral using composite simpson's rule. This will changed to Gaussian Quadrature in the future.

```{r}
# Creating simpsons rule function
composite_approx <- function(f, a, b, n) {
  
  ## Error checking code
  if (is.function(f) == FALSE) {
    stop('f must be a function with one parameter (variable)')
  }
  
  ## General formula
  h <- (b - a)/n
  
  ## Setting parameters
  xn <- seq.int(a, b, length.out = n + 1)
  xn <- xn[-1]
  xn <- xn[-length(xn)]
  
  ## Approximating
  integ_approx <- (h/3)*(f(a) + 2*sum(f(xn[seq.int(2, length(xn), 2)])) + 
                     4*sum(f(xn[seq.int(1, length(xn), 2)])) + 
                     f(b))
  
  ## Returning result
  return(integ_approx)
  
}
```

Next, I define the function which takes in a set of functional observations, a basis for the coefficient function, $\beta(t)$, and an indicator for which observation we are looking at in the set of functional observations.

```{r}
fda_integ_approximator <- function(beta_basis, func_obs, k) {
  ### First, I create formulas for the functional observation, x(t)
  
  ## Initializing
  updated_names <- NULL
  updated_internal_name <- NULL
  
  ## Looping to create coefficients
  for (i in 1:length(func_obs$coefs[1,])) {
    for (j in 1:length(func_obs$basis$names)) {
      if(func_obs$basis$names[j] == "const"){
        updated_internal_name <- paste0(updated_internal_name, " + ", func_obs$coefs[j,i])
      } else {
        updated_internal_name <- paste0(updated_internal_name, " + ", func_obs$coefs[j,i], "*", func_obs$basis$names[j])
      }
    }
    updated_internal_name <- substring(updated_internal_name, 4)
    updated_names[i] <- updated_internal_name
    updated_internal_name <- NULL
  }
  
  ### Next, I create an update on the beta names
  
  ## Initializing
  updated_beta_names <- NULL
  
  ## Looping
  for (i in 1:length(beta_basis$names)) {
    updated_beta_names <- paste0(beta_basis$names)
  }
  
  ### Next, I put together the previous results
  
  ## Cleaning up data set after expanding
  basis_combinations = expand.grid(updated_beta_names, updated_names[k])
  basis_combinations = basis_combinations[basis_combinations$Var1 != "const", ]
  basis_combinations = basis_combinations[basis_combinations$Var2 != "const", ]
  
  ## Initializing updated form
  final_form <- NULL
  
  ### Now, getting proper combinations
  for (i in 1:length(basis_combinations$Var1)) {
    final_form[i] <- paste0(basis_combinations[i, 1], "*", "(", updated_names[k], ")")
  }
  
  ### Replacing sin1, cos1 type of things
  for (j in 1:length(basis_combinations$Var1)) {
    for (i in 1:100) {
      if(grepl(paste0("sin", 100 - i), final_form[j])){
        final_form[j] <- gsub(paste0("sin", 100 - i), paste0("sin", "(", 100 - i, "*", "365", "*", "x)"), final_form[j])
      }
      if(grepl(paste0("cos", 100 - i), final_form[j])){
        final_form[j] <- gsub(paste0("cos", 100 - i), paste0("cos", "(", 100 - i, "*", "365", "*", "x)"), final_form[j])
      }
    }
  }
  
  ## Taking care of const
  const_calc <- updated_names[k]
  for (i in 1:100) {
    if(grepl(paste0("sin", 100 - i), const_calc)){
      const_calc <- gsub(paste0("sin", 100 - i), paste0("sin", "(", 100 - i, "*", "365", "*", "x)"), const_calc)
    }
    if(grepl(paste0("cos", 100 - i), const_calc)){
      const_calc <- gsub(paste0("cos", 100 - i), paste0("cos", "(", 100 - i, "*", "365", "*", "x)"), const_calc)
    }
  }
  
  # Creating function
  const_func <- function(x){
    a = eval(parse(text = const_calc))
    return(a)
  }
  
  # Doing integral
  const_int <- composite_approx(const_func, 0, 365, 5000)
  
  # Initializing
  integral_evals <- c()
  
  # Looping to get all evals
  for (r in 1:length(basis_combinations$Var1)) {
    
    final_func <- function(x){
      a = eval(parse(text = final_form[r]))
      return(a)
    }
    
    integral_evals[r] <- composite_approx(final_func, 0, 365, 5000)
  }
  
  # Putting together
  final_results <- c(const_int, integral_evals)
  
  final_results <- c(scale(final_results))
  
  return(final_results)
  
}
```

Now, let's read in the data set and set it up so that it can be used with the above functions. This code is borrowed from FDA in R and MATLAB by James Ramsay. The toy example to be used is using the functional observations associated with temperature across 35 cities to predict the scalar response, average annual precipitation:

```{r}
# obtain the annual precipitation for 35 cities
annualprec = apply(daily$precav,2,mean)

# Define the 65 Fourier basis functions
tempbasis65  = create.fourier.basis(c(0,365), 65)
timepts = seq(1, 365, 1)
temp_fd = Data2fd(timepts, daily$tempav, tempbasis65)
temp_fd$coefs <- scale(temp_fd$coefs)

# Creating vector to store information
templist = vector("list",2)
templist[[1]] = rep(1,35) 
templist[[2]] = temp_fd 

# create a constant basis for the intercept
conbasis = create.constant.basis(c(0,365))
nbasis = 11
betabasis5 = create.fourier.basis(c(0,365), nbasis)
betalist1  = vector("list",2)
betalist1[[1]] = conbasis
betalist1[[2]] = betabasis5
```

Now, let's test out the integral approximation function for the functional observation for Resolute:

```{r}
fda_integ_approximator(betabasis5, temp_fd, 35)
```

To be clear, these are the integrations associated with:
$$ \begin{align}
    z_{1}^{(1)} &= \sigma(\int w_{1}(t)x_{1}(t)dt + b_{1})\\
    &= \sigma(\int \sum c_{i}\psi(t)x_{1}(t)dt + b_{1})\\
    &= \sigma(\sum c_{i}\underbrace{\int\psi(t)x_{1}(t)dt}_{Approximated} + b_{1})
\end{align} $$

So, to be clear, that sum has 11 terms and the integral values are the ones above.

Let's first set up the functional neural network and then compare it with a functional regression afterwards:

```{r}
### Setting seed
set.seed(25)

############## Functional Neural Network ############## 

## Activation Function
reLu <- function(x) {
  return(max(0, x))
}

## Derivative of the activation
reLu_deriv <- function(x) {
  return()
}

## Loss Function
MSE <- function(neural_net) {
  return(mean((neural_net$y - neural_net$output)^2))
}

## Initializing weights

#### W_1(t) Weights
layer_weights_1 <- matrix(data = runif(11*1), nrow = 34, ncol = 11)

### OTHER WEIGHTS
layer_weights_2 <- c(runif(length(annualprec) - 1))
layer_bias_2 <- c(runif(length(length(annualprec) - 1)))
layer_bias_1 <- c(runif(length(length(annualprec) - 1)))

## Setting up neural network list
neuralnet_info <- list(
  
  # Input observations
  input = temp_fd,
  
  #### W_1(t) Weights
  layer_weights_1 = layer_weights_1,
  
  ## Otherweights
  layer_bias_1 = layer_bias_1,
  layer_weights_2 = layer_weights_2,
  layer_bias_2 = layer_bias_2,
  
  ## Output
  y = annualprec[-35],
  
  ## Predictions
  output = matrix(rep(0, 34), ncol = 34)
)


## Forward pass
forward_pass <- function(neural_net, nobs) {
  
  for (i in 1:nobs) {
    
    # Layer 1 activations
    neural_net$layer1[i] <- c(fda_integ_approximator(betabasis5, temp_fd, i)%*%c(neural_net$layer_weights_1[i,]) + 
                                     layer_bias_1)
  }
  
  # Output activations
  neural_net$output <- c(neural_net$layer1 * neural_net$layer_weights_2 + 
                                   layer_bias_2)
  
  return(neural_net)
}

deriv_weights1 <- matrix(ncol = 11, nrow = 34)


## Backpropagation
grad_descent <- function(neural_net, nobs, f){
  
  ## Easier derivative first
  # weights closer to the output layer
  deriv_weights2 <- 2*(neural_net$y - neural_net$output)*neural_net$layer1
  
  ## Backpropagating to first layer
  # Applied chain rule here
  for (i in 1:nobs) {
    
    # Layer 1 activations
    deriv_weights1[i,] <- 2*(neural_net$y - neural_net$output)[i]*fda_integ_approximator(betabasis5, temp_fd, i)*layer_weights_2[i]
    
  }
  
  
  
  
  ## Now need to do bias derivatives
  deriv_bias2 <- 2*(neural_net$y - neural_net$output)
  
  deriv_bias1 <- 2*(neural_net$y - neural_net$output)*layer_weights_2*neural_net$layer1
  
  ## Adaptive learning rate
  if(f < 6){learn_rate = 0.1}
  if(f >= 6){learn_rate = 0.01}
  if(f > 11){learn_rate = 0.001}
  
  ## Weight update using derivative
  for (i in 1:nobs) {
    # Layer 1 activations
    neural_net$layer_weights_1[i,] <- neural_net$layer_weights_1[i,] + learn_rate*c(scale(deriv_weights1[i,]))
    
  }
  
  neural_net$layer_weights_2 <- neural_net$layer_weights_2 + learn_rate*c(scale(deriv_weights2))
  neural_net$layer_bias_1 <- neural_net$layer_bias_1 + learn_rate*c(scale(deriv_bias1))
  neural_net$layer_bias_2 <- neural_net$layer_bias_2 + learn_rate*c(scale(deriv_bias2))
  
  # Returning updated information
  return(neural_net)
  
}

```

Above, I defined the forward and backward pass of the functional neural network and initialized it with some random weights generated from the uniform distribution.

Let's take a look at the error rate right now, before training the model and compare it with the functional regression above.

```{r}
## Error Rate after no iterations
mean((neuralnet_info$output - annualprec[-35])^2)
```

This is much worse at the moment compared with MSE of 0.4844295 in the traditional functional regression model that you see I did below. Now, we train the model:

```{r}
## Epochs
epoch_num <- 10

## Initializing loss vector
lossData <- data.frame(epoch = 1:epoch_num, MSE = rep(0, epoch_num))

## Training Neural Net
for (f in 1:epoch_num) {
  
  # Foreward iteration
  neuralnet_info <- forward_pass(neuralnet_info, 34)
  
  # Backward iteration
  neuralnet_info <- grad_descent(neuralnet_info, 34, f)
  
  # Storing loss
  lossData$MSE[f] <- MSE(neuralnet_info)
  
}
```

```{r}
## Error Rate after 10 iterations
FunctionalNN_MSE <- mean((neuralnet_info$output - annualprec[-35])^2)
```

Now, I will save the MSE and a out of bag Resolute prediction for later as a fun surprise to see if it did better. Let's first run the functional linear regression model to predict for Resolute.

```{r}
#### PREDICTING FOR RESOLUTE

# pick out data and 'new data'
dailydat <- daily$precav[,1:34]
dailytemp <- daily$tempav[,1:34]
dailydatNew <- daily$precav[,35]
dailytempNew <- daily$tempav[,35]

# set up response variable
annualprec1 <- apply(dailydat, 2, mean)

# create basis objects for and smooth covariate functions
tempbasis <- create.fourier.basis(c(0,365),65)
tempSmooth <- smooth.basis(day.5,dailytemp,tempbasis)
tempfd <- tempSmooth$fd

# create design matrix object
templist <- vector("list",2)
templist[[1]] <- rep(1,34)
templist[[2]] <- tempfd

# create constant basis (for intercept) and
# fourier basis objects for remaining betas
conbasis <- create.constant.basis(c(0,365))
betabasis <- create.fourier.basis(c(0,365), 11)
betalist <- vector("list",2)
betalist[[1]] <- conbasis
betalist[[2]] <- betabasis

# regress
annPrecTemp <- fRegress(annualprec1, templist, betalist)

# Checking mse
annualprechat1 = annPrecTemp$yhatfdobj
mean((annualprechat1 - annualprec1[-c(35)])^2)

# create basis objects for and smooth covariate functions for new data
tempSmoothNew <- smooth.basis(day.5,dailytempNew,tempbasis)
tempfdNew <- tempSmoothNew$fd

# create design matrix object for new data
templistNew <- vector("list",2)
templistNew[[1]] <- rep(1,1)
templistNew[[2]] <- tempfdNew

# convert the intercept into an fd object
onebasis <- create.constant.basis(c(0,365))
templistNew[[1]] <- fd(matrix(templistNew[[1]],1,1), onebasis)

# set up yhat matrix (in our case it's 1x1)
yhatmat <- matrix(0,1,1)

# loop through covariates
p <- length(templistNew)
for(j in 1:p){
  xfdj       <- templistNew[[j]]
  xbasis     <- xfdj$basis
  xnbasis    <- xbasis$nbasis
  xrng       <- xbasis$rangeval
  nfine      <- max(501,10*xnbasis+1)
  tfine      <- seq(xrng[1], xrng[2], len=nfine)
  deltat     <- tfine[2]-tfine[1]
  xmat       <- eval.fd(tfine, xfdj)
  betafdParj <- annPrecTemp$betaestlist[[j]]
  betafdj    <- betafdParj$fd
  betamat    <- eval.fd(tfine, betafdj)
  # estimate int(x*beta) via trapezoid rule
  fitj       <- deltat*(crossprod(xmat,betamat) - 
                          0.5*(outer(xmat[1,],betamat[1,]) +
                                 outer(xmat[nfine,],betamat[nfine,])))
  yhatmat    <- yhatmat + fitj
}

## Checking prediction accuracy
resolute_prediction_fRegrss <- (yhatmat - mean(dailydatNew))^2

```

Here, we see the MSE is 0.4844295 for the functional regression model (as mentioned earlier).

```{r}
resolute_prediction_fRegrss
```

And the squared error for the Resolute prediction is $\approx$ 0.2417. 

Now, let's take a look at the MSE for the functional neural network:

```{r}
## Error Rate after 10 iterations
FunctionalNN_MSE
```

Wow! we have a miniscule MSE now of: 0.03043234. Here is a plot of how the loss changes as we move through the epochs:

```{r}
## Plotting Loss
lossData %>% 
  ggplot(aes(x = epoch, y = MSE)) + 
  geom_line(size = 1.25, color = "steelblue2") +
  theme_bw() +
  labs(x = "Epoch #", y = "MSE") +
  ggtitle("Change in Loss - Functional Neural Net") +
  theme(plot.title = element_text(hjust = 0.5))
```

Let's now do a prediction on Resolute using this trained network. 

```{r}
resolute_layer1 <- c(fda_integ_approximator(betabasis5, temp_fd, 35)%*%colMeans(neuralnet_info$layer_weights_1) + 
                            mean(neuralnet_info$layer_bias_1))


resolute_pred <- c(resolute_layer1*mean(neuralnet_info$layer_weights_2) + 
              mean(neuralnet_info$layer_bias_2))

(resolute_error <- (resolute_pred - mean(dailydatNew))^2)
```

This out of bag error of 0.00006251 is much lower than what we had for the functional regression model of 0.241762! Let's now plot the beta coefficient function uncovered in the functional neural network:

```{r}
#### Plotting coefficient function for the first layer
final_beta_function <- function(x){
  value <- 0.9472645 + 0.2540166*sin(365*x) - 0.1838267*cos(365*x) +
    0.4881373*sin(2*365*x) + 0.4477605*cos(2*365*x) + 0.4955367*sin(3*365*x) +
    0.4886242*cos(3*365*x) + 0.5034172*sin(4*365*x) + 0.47760322*cos(4*365*x) +
    0.4931472*sin(5*365*x) + 0.4954288*cos(5*365*x)
  return(value)
}

beta_coef <- data.frame(time = seq(1, 365, 15), 
                        beta_evals = final_beta_function(seq(1, 365, 15)))

beta_coef %>% 
  ggplot(aes(x = time, y = beta_evals)) +
  geom_smooth(method = "loess", se = F) +
  theme_bw() +
  ggtitle("Coefficient Function") +
  xlab("Day") +
  ylab("Beta for Temperature") +
  theme(plot.title = element_text(hjust = 0.5))
```

We can also look at the residuals from the functional neural network:

```{r}
data.frame(yhat = neuralnet_info$output, 
                             res = neuralnet_info$y - neuralnet_info$output) %>% 
  ggplot(aes(x = yhat, y = res)) +
  geom_point(color = "steelblue2") +
  geom_hline(yintercept = 0, color = "blue") +
  theme_bw() +
  ggtitle("Residual Plot") +
  xlab("Fitted Values") +
  ylab("Residuals") +
  theme(plot.title = element_text(hjust = 0.5))
```

And lastly, we can look at the fitted vs. true values:

```{r}
data.frame(yhat = neuralnet_info$output, 
                             y = neuralnet_info$y) %>% 
  ggplot(aes(x = yhat, y = y)) +
  geom_point(color = "steelblue2") +
  theme_bw() +
  xlab("Fitted Values") +
  ylab("Actual Values") +
  theme(plot.title = element_text(hjust = 0.5))
```

Lastly, we can do a comparison with a regular feed-forward neural network with the same conditions as the functional neural network:

```{r}
### Neural Network Comparison
annualprec = apply(daily$precav,2,mean)

## Creating data frame
temp_prec_df <- data.frame(cbind(annualprec, t(daily$tempav)))

## Loading package
library(neuralnet)

## Scaling data
temp_prec_df_scaled <- as.data.frame(scale(temp_prec_df))

## Running neural network
NN = neuralnet(annualprec ~ ., 
                temp_prec_df_scaled, 
                hidden = 1 , 
                linear.output = T,
               rep = 10)

## Looking at error rate
NN$result.matrix[1]
```

This error rate is higher than the one we experienced using the functional neural network (albeit slower, but also not optimized as of yet).

That completes the code thus far for this type of model. This is very specific to this data set and was for the purposes of example. This will be developed generally in the future. Also, this was limited to feed-forward neural networks with scalar responses and functional covariates - extensions will be made to the two other types of functional linear models along with a crossover to residual and recurrent neural networks AND neural ordinary differential equations.











